
"""
Some useful base classes for writing custom tests of pbsmrtpipe output.
"""

from unittest import SkipTest
import operator as OP
import traceback
import logging
import json
import os
import sys

from pbcommand.pb_io.tool_contract_io import load_resolved_tool_contract_from
from pbcommand.pb_io.report import load_report_from_json, dict_to_report
from pbcommand.models import FileTypes, DataStoreFile

import pbsmrtpipe.testkit.core.base

from pysiv2.io.entrypoints import EntryPoints
from pysiv2.io.datastore import DataStore
from pysiv2.custom import utils as u

log = logging.getLogger(__name__)


def _file_test_id(task_id, file_type):
    return "{t}_{e}".format(t="_".join(task_id.split(".")), e=file_type.ext)


class TestBase(pbsmrtpipe.testkit.core.base.TestBase):
    # XXX subclasses may need to override this, e.g. if they specifically need
    # to retrieve chunked files
    FORCE_USE_LOCAL_DATASTORE = False

    @classmethod
    def setUpClass(cls):
        super(TestBase, cls).setUpClass()
        cls.entrypoints = EntryPoints.from_job_path(cls.job_dir)
        if cls.service_access_layer is None or cls.FORCE_USE_LOCAL_DATASTORE:
            cls.datastore = DataStore.from_job_path(cls.job_dir)
        else:
            ds = cls.service_access_layer.get_analysis_job_datastore(cls.job_id)
            cls.datastore = DataStore(ds)
            ep = cls.service_access_layer.get_analysis_job_entry_points(
                cls.job_id)
            # FIXME
            #cls.entrypoints = EntryPoints(ep)


class TestFilesWereGenerated(TestBase):

    """
    This is basically just a template for verifying (and potentially
    validating) that expected files were generated, by means of simple
    subclasses that override TASK_IDS and FILE_TYPES (which must be lists of
    the same length).
    """

    TASK_IDS = []
    FILE_TYPES = []

    class __metaclass__(type):

        def __new__(cls, classname, bases, classdict):
            for task_id, file_type in zip(classdict["TASK_IDS"],
                                          classdict["FILE_TYPES"]):
                test_f = TestFilesWereGenerated.make_func(task_id, file_type)
                func_name = "test_{i}".format(
                    i=_file_test_id(task_id, file_type))
                test_f.__name__ = func_name
                test_f.__doc__ = """\
                Test that the datastore contains a file of type {f} generated
                by task {t}.
                """.format(t=task_id, f=file_type)
                classdict[func_name] = test_f
            return type.__new__(cls, classname, bases, classdict)

    @staticmethod
    def make_func(task_id, file_type):
        def test(self):
            fid = "{t}-{f}".format(t=task_id, f=file_type.file_type_id)
            fn = self.datastore._get_task_file(task_id, file_type.file_type_id)
            self.assertTrue(fn is not None, "missing %s" % fid)
            ftid = _file_test_id(task_id, file_type)
            if hasattr(self, "verify_%s_file" % ftid):
                getattr(self, "verify_%s_file" % ftid)(fn)
            return True
        return test


class TestValuesLoader(TestBase):

    """
    Base class for tests that load a JSON dictionary of expected values.
    """
    JSON_FILE = "test_values.json"
    HAVE_TEST_VALUES = False
    _FAILURE_TRACEBACK = None

    @classmethod
    def setUpClass(cls):
        """
        Load expected values from the test's JSON dictionary.
        This is tolerant of errors, which will be re-raised in setUp().
        """
        super(TestValuesLoader, cls).setUpClass()
        json_file = os.path.join(cls.job_dir, cls.JSON_FILE)
        if not os.path.isfile(json_file):
            json_file = os.path.join(os.getcwd(), cls.JSON_FILE)
            if not os.path.isfile(json_file):
                log.warning("Missing %s" % cls.JSON_FILE)
                return
            else:
                log.info("Using test values in %s" % json_file)
        log.info('about to load test_values')
        try:
            with open(json_file, 'r') as f:
                cls.test_values = u.unicode_to_string(json.load(f))
        except (IOError, OSError, ValueError) as e:
            log.error("Error loading %s: %s" % (json_file, e))
            tb = traceback.format_exc()
            cls._FAILURE_TRACEBACK = "%s:\n%s" % (str(e), tb)
        else:
            cls.HAVE_TEST_VALUES = True

    def setUp(self):
        super(TestValuesLoader, self).setUp()
        if self._FAILURE_TRACEBACK is not None:
            self.fail("setUpClass failed:\n%s" % self._FAILURE_TRACEBACK)


class TestStatisticsBase(TestValuesLoader):

    """
    This base class allows us to very quickly add tests that compare actual
    results against expected values.  The assumption is that a testkit job has
    a JSON file that looks something like this:

    {
        'ccs': {
            'number_of_ccs_reads': 100
        },
        'mapping': {
            'number_of_aligned_reads__ge': 95
        }
    }

    which can specify exact values, or use operator suffixes (inspired by
    Django) for looser comparisons.  It is left to subclasses to specify which
    keys are expected in the METRIC_IDS class attribute, and to extract the
    corresponding metrics from the job results.
    """

    JSON_SCOPE = None
    TEST_ID = None
    METRIC_IDS = []
    DEFAULT_VALUES = {}
    SKIP_TESTS = {} # in case we need to bail out for some reason

    # dynamically define test cases at the time of class definition.  in
    # the vanilla unittest framework this could be deferred until later, but
    # for our pbsmrtpipe tests the new methods need to be visible on import
    class __metaclass__(type):

        def __new__(cls, classname, bases, classdict):
            test_id = classdict['TEST_ID']
            # XXX here follows some hackery to make the docstring correspond to
            # the structure of the JSON file - currently this only affects the
            # report-based tests
            json_scope = test_id
            if classdict.get('JSON_SCOPE', None) is not None:
                json_scope = "{s}.{i}".format(s=classdict['JSON_SCOPE'],
                                              i=test_id)
            else:
                for base_cls in bases:
                    if getattr(base_cls, "JSON_SCOPE", None) is not None:
                        json_scope = "{s}.{i}".format(s=base_cls.JSON_SCOPE,
                                                      i=test_id)
                        break
            for stat_key in classdict['METRIC_IDS']:
                test_f = TestStatisticsBase.make_func(stat_key)
                test_f.__doc__ = """\
            Test that the metric ``{s}.{k}`` is within the range of acceptable
            values specified in ``test_values.json``.  Default behavior is to
            check equality, but this can be
            modified by adding a suffix to the key in ``test_values.json``, for
            example ``{k}__lt`` to specify that the metric should be less than
            the given value.
            """.format(s=json_scope, k=stat_key)
                method_name = "test_%s_%s" % (test_id, stat_key)
                test_f.__name__ = method_name
                classdict[method_name] = test_f
            return type.__new__(cls, classname, bases, classdict)


    @staticmethod
    def make_func(stat_key):
        def test(self):
            if stat_key in self.SKIP_TESTS:
                raise SkipTest(self.SKIP_TESTS[stat_key])
            log.info("Test %s" % stat_key)
            return self._compare_stats(stat_key)
        return test

    @classmethod
    def setUpClass(cls):
        """
        Prepare the test class for use, mainly populating various metrics.
        This is tolerant of errors, which will be re-raised in setUp().
        """
        super(TestStatisticsBase, cls).setUpClass()
        cls.expected_values = {}
        cls.metric_dict = {}
        try:
            cls.getMetrics()
        except Exception as e:
            log.error(str(e))
            if cls._FAILURE_TRACEBACK is None:
                cls._FAILURE_TRACEBACK = traceback.format_exc()
        cls.getTestValues()

    @classmethod
    def getTestValues(cls):
        if cls.HAVE_TEST_VALUES:
            cls.expected_values = cls.test_values.get(cls.TEST_ID, {})

    @classmethod
    def getMetrics(cls):
        """
        Subclasses should implement this function to load the appropriate
        metrics for the run, typically by mining the output files.
        """
        raise NotImplementedError("must implement in subclasses")

    def _get_stat(self, stat_id):
        return self.metric_dict.get(stat_id, None)

    def _expected_values_and_operators(self, stat_id):
        if not self.HAVE_TEST_VALUES:
            raise StopIteration()
        elif stat_id in self.expected_values:
            yield self.expected_values[stat_id], OP.eq
        else:
            n_ops = 0
            for key, value in self.expected_values.iteritems():
                for op in ["lt", "gt", "le", "ge", "eq"]:
                    if key == (stat_id + "__" + op):
                        val = self.expected_values[key]
                        n_ops += 1
                        yield val, getattr(OP, op)
            if n_ops == 0 and stat_id in self.DEFAULT_VALUES:
                yield self.DEFAULT_VALUES[stat_id], OP.eq

    def _compare_stats(self, stat_id):
        value = self._get_stat(stat_id)
        n_tested = 0
        for expected, operator in self._expected_values_and_operators(stat_id):
            eqn = "%s .%s. %s" % (value, operator.__name__, expected)
            log.info("Comparing values of %s: %s" % (stat_id, eqn))
            self.assertTrue(operator(value, expected),
                            "%s: ! %s" % (stat_id, eqn))
            n_tested += 1
        if n_tested == 0:
            raise SkipTest("No expected values found for %s" % stat_id)


class TestReportStatistics(TestStatisticsBase):

    """
    This base class builds on TestStatisticsBase, using the output of pbreports
    instead of diving into the underlying results.
    """

    JSON_SCOPE = "reports"
    REPORT_ID = None
    TEST_ID = None
    METRIC_IDS = []

    @classmethod
    def getTestValues(cls):
        if cls.HAVE_TEST_VALUES:
            cls.expected_values = cls.test_values.get("reports", {}).get(cls.TEST_ID, {})

    @classmethod
    def getMetrics(cls):
        cls.report = cls.getReport()
        cls.report_stats = cls.report.attributes

    @classmethod
    def getReport(cls):
        if cls.service_access_layer is None:
            return cls.datastore.get_report(cls.REPORT_ID)
        else:
            report_id = cls.REPORT_ID
            if isinstance(cls.REPORT_ID, basestring):
                report_id = set([cls.REPORT_ID])
            # load report from services, not raw file
            for rpt_info in cls.service_access_layer.get_analysis_job_reports(
                cls.job_id):
                file_info = rpt_info['dataStoreFile']
                rpt = load_report_from_json(file_info.path)
                if rpt.id in report_id:
                    return rpt
            raise IOError("Can't find report with ID {i}".format(
                          i=" OR ".join(sorted(list(report_id)))))

    def _get_stat(self, stat_id):
        for stat in self.report_stats:
            if stat.id == stat_id:
                return stat.value
        return super(TestReportStatistics, self)._get_stat(stat_id)

    def _get_table_row(self, table_id, column_id):
        for table in self.report.tables:
            if table.id == table_id:
                for column in table.columns:
                    if column.id == column_id:
                        return column.values
        raise KeyError("Table column {r}.{t}.{c} not found.".format(
            r=self.report.id, t=table_id, c=column_id))


def setUpFindAlignments(self):
    """
    Identify the final AlignmentSet from resequencing jobs and save the
    path to the attribute alignment_file_name.
    """
    self.alignment_file_name = None
    self.consolidated_alignment_file_name = None
    pbalign_files = []
    for file_id, file_info in self.datastore.get_file_dict().iteritems():
        if file_info.is_chunked:
            continue
        if file_info.file_type_id == FileTypes.DS_ALIGN.file_type_id:
            if "consolidate_alignments" in file_info.file_id:
                self.consolidated_alignment_file_name = file_info.path
                break
            # FIXME special case, get rid of this eventually
            elif ("pbalign.tasks.pbalign" in file_info.file_id or
                  "pysiv2.tasks.pbalign_unrolled" in file_info.file_id):
                pbalign_files.append(file_info.path)
    if self.consolidated_alignment_file_name is not None:
        self.alignment_file_name = self.consolidated_alignment_file_name
    elif len(pbalign_files) > 0:
        assert len(pbalign_files) == 1
        self.alignment_file_name = pbalign_files[0]


class TestResequencingOutput(TestBase):

    """Base class for tests that inspect resequencing output"""

    def setUp(self):
        setUpFindAlignments(self)


class LoadResolvedToolContractMixin(object):

    @classmethod
    def loadRtcs(cls):
        cls.tasks_dir = os.path.join(cls.job_dir, "tasks")
        task_contents = os.listdir(cls.tasks_dir)
        cls.resolved_tool_contracts = []
        for task_name in task_contents:
            task_dir = os.path.join(cls.tasks_dir, task_name)
            if not os.path.isdir(task_dir):
                continue
            task_id, job_id = task_name.split("-")
            rtc_json = os.path.join(task_dir, "resolved-tool-contract.json")
            if not os.path.isfile(rtc_json):
                log.warn("Can't find %s" % rtc_json)
                continue
            rtc = load_resolved_tool_contract_from(rtc_json)
            cls.resolved_tool_contracts.append(rtc)
