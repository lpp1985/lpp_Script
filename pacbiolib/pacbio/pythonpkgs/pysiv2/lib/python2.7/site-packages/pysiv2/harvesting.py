
"""
Harvest testkit job configs (and associated batch .txt files) to produce
various documents describing tests.
"""

from collections import namedtuple, OrderedDict
import ConfigParser
import json
import re
import os.path as op
import os
import sys

from pbcore.io import SubreadSet
from pbsmrtpipe.testkit.service_runner import pipeline_id_from_testkit_cfg
from pbsmrtpipe.loader import load_all_installed_pipelines

testkit_job = namedtuple("testkit_job", ("job_id", "description", "pipeline",
                                         "entry_points", "pysiv2_tests",
                                         "data_type", "data_stats",
                                         "test_metrics"))
testkit_job_p4 = namedtuple("testkit_job_p4",
                            ("job_id", "description", "pipeline",
                             "entry_points", "pysiv2_tests",
                             "data_type", "data_stats", "test_metrics",
                             "p4_path"))
data_stats = namedtuple("data_stats", ("n_zmws", "n_subreads", "n_bases"))

EID_LABELS = {
    "eid_subread": "SubreadSet",
    "eid_barcode": "BarcodeSet",
    "eid_ref_dataset": "ReferenceSet",
    "eid_hdfsubread": "HdfSubreadSet",
    "eid_ref_fasta": "FASTA",
    "rs_movie_xml": "RSII movie metadata XML"
}

OPS = {
    "eq": "==",
    "gt": ">",
    "ge": ">=",
    "lt": "<",
    "le": "<="
}


def _get_test_metrics(test_values_json):
    test_metrics = []
    if op.isfile(test_values_json):
        with open(test_values_json) as f:
            test_values = json.load(f)
            reports = test_values.get("reports", {})
            for report_id, report in reports.iteritems():
                for metric_id, value in report.iteritems():
                    if not isinstance(value, (int, float)):
                        continue
                    fields = metric_id.split("__")
                    operator = "=="
                    if len(fields) == 2:
                        metric_id = fields[0]
                        operator = OPS[fields[1]]
                    test_metrics.append("{r}.{m} {o} {v}".format(
                                        r=report_id, m=metric_id, o=operator,
                                        v=value))
    return test_metrics

def get_testkit_cfg_data(file_name):
    """
    Extract test metadata from a testkit.cfg file.
    """
    cfg = ConfigParser.ConfigParser()
    cfg.read(file_name)
    try:
        tests = cfg.get("tests", "pysiv2.custom").split(", ")
    except ConfigParser.NoOptionError:
        tests = []
    entry_points = []
    for ep_key in cfg.options("entry_points"):
        entry_points.append((ep_key, cfg.get("entry_points", ep_key)))
    data_type = get_sequencing_chemistry(entry_points)
    pipeline_id = pipeline_id_from_testkit_cfg(file_name)
    pipelines = load_all_installed_pipelines()
    test_values_json = op.join(op.dirname(file_name), "test_values.json")
    test_metrics = _get_test_metrics(test_values_json)
    return testkit_job(
        cfg.get("pbsmrtpipe:pipeline", "id"),
        cfg.get("pbsmrtpipe:pipeline", "description"),
        pipelines[pipeline_id].display_name,
        entry_points,
        tests,
        get_sequencing_chemistry(entry_points),
        get_data_stats(entry_points),
        test_metrics)


def get_testkit_json_data(file_name):
    """
    Extract test metadata from a testkit_cfg.json file.
    """
    with open(file_name) as json_f:
        cfg = json.load(json_f)
        assert cfg.get('jobType', "pbsmrtpipe") == "pbsmrtpipe"
        entry_points = []
        for ep in cfg['entryPoints']:
            entry_points.append((ep['entryId'], ep['path']))
        tests = []
        for module, test_names in cfg["pythonTests"].iteritems():
            tests.extend(test_names)
        data_type = get_sequencing_chemistry(entry_points)
        pipeline_id = cfg['pipelineId']
        pipelines = load_all_installed_pipelines()
        test_values_json = op.join(op.dirname(file_name), "test_values.json")
        test_metrics = _get_test_metrics(test_values_json)
        return testkit_job(
            cfg['testId'],
            cfg['description'],
            pipelines[pipeline_id].display_name,
            entry_points,
            tests,
            get_sequencing_chemistry(entry_points),
            get_data_stats(entry_points),
            test_metrics)


def make_rst_table(rows, headers=None):
    """
    Construct RST syntax for a generic table.
    """
    _rows = list(rows)
    if headers is not None:
        assert len(headers) == len(rows[0])
        _rows.append(headers)
    widths = [max([len(_rows[j][i]) for j in range(len(_rows))])
              for i in range(len(_rows[0]))]
    format_str = "| " + \
        " | ".join(["%-{:d}s".format(x) for x in widths]) + " |"
    sep_str = "+" + "+".join(["-" * (x + 2) for x in widths]) + "+"
    table = [sep_str]
    if headers is not None:
        table.append(format_str % tuple(headers))
        table.append(re.sub("-", "=", sep_str))
    for row in rows:
        table.append(format_str % tuple(row))
        table.append(sep_str)
    return "\n".join(table)


def get_sequencing_chemistry(entry_points, include_system_type=True):
    """
    Given a list of entry points (eid, path), extract the sequencing chemistry
    (and optionally system name) as a human-readable string.
    """
    chemistries = set()
    is_sequel = is_rsii = False
    for eid, path in entry_points:
        if eid == "eid_subread" and op.isfile(path):
            ds = SubreadSet(path)
            for bam in ds.resourceReaders():
                for rg in bam.readGroupTable:
                    chemistries.add(rg.SequencingChemistry)
                    if rg.SequencingChemistry.startswith("S"):
                        is_sequel = True
                    else:
                        is_rsii = True
    if len(chemistries) == 0:
        return "NA"
    chemistry_str = "; ".join(sorted(list(chemistries)))
    if include_system_type:
        fmt = "{s} ({c})"
        if is_sequel and is_rsii:
            return fmt.format(s="Mixed", c=chemistry_str)
        elif is_sequel:
            return fmt.format(s="Sequel", c=chemistry_str)
        elif is_rsii:
            return fmt.format(s="RSII", c=chemistry_str)
        else:
            raise ValueError("Can't determine system type for {c}".format(
                             c=chemistry_str))
    return chemistry_str


def get_data_stats(entry_points):
    """
    Get basic metrics for input dataset (assumed to be a SubreadSet).
    """
    for eid, path in entry_points:
        if eid == "eid_subread" and op.isfile(path):
            ds = SubreadSet(path)
            n_zmws = 0
            for bam in ds.resourceReaders():
                n_zmws += len(set(bam.pbi.holeNumber))
            return data_stats(n_zmws, ds.numRecords, ds.totalLength)
    return data_stats("NA", "NA", "NA")


def add_p4_path(p4_root, test, cfg_file):
    """
    Utility function to generate the full Perforce repo path for a config file
    given a prefix path.
    """
    p4_path = p4_root + "/" + re.sub("^\.\/", "", cfg_file)
    fields = list(tuple(test)) + [p4_path]
    return testkit_job_p4(*fields)


def _get_subread_set(entry_points):
    for eid, path in entry_points:
        if eid == "eid_subread" and op.isfile(path):
            return path
    return ""


def format_data_stats(s):
    return "{z} ZMWs, {s} subreads, {b} bases".format(z=s.n_zmws,
                                                      s=s.n_subreads,
                                                      b=s.n_bases)


XLS_HEADERS = ["Sub-System", "Component", "Application", "Data Type",
               "Description",
               "Test Requirements", "Automated", "Test Level", "Pass/Fail",
               "Justification", "Dataset(s)", "Input Size", "Test ID", "Perforce Path"]
LOCK_CELLS = set([0, 1, 2, 3, 4, 5, 7, 9, 10, 11, 12, 13])
COL_WIDTHS = [20, 20, 40, 20, 80, 60, 15, 15, 20, 20, 200, 40, 20, 100]


def write_excel_test_plan(tests, file_name, service_tests=()):
    """
    Generate a test plan acceptable to TPTB in Excel format.
    """
    import xlsxwriter
    service_tests = set(service_tests)
    wb = xlsxwriter.Workbook(file_name)
    ws = wb.add_worksheet("Analysis Tests")
    unlocked = wb.add_format({'locked': 0})
    ws.protect()
    for j, width in enumerate(COL_WIDTHS):
        ws.set_column(j, j, width)
    for j, header in enumerate(XLS_HEADERS):
        ws.write(0, j, header)
    i = 1
    for t in tests:
        def _write_row(row_):
            for j, item in enumerate(row_):
                if not j in LOCK_CELLS:
                    ws.write(i, j, item, unlocked)
                else:
                    ws.write(i, j, item)
        level = "CMDLINE"
        if t.job_id in service_tests:
            level = "SERVICES+CMDLINE"
        row = ["SMRT Link - SA", "End-to-end Pipeline", t.pipeline, t.data_type,
               t.description, "Job runs to completion without errors", "YES",
               level, "", "", _get_subread_set(t.entry_points),
               format_data_stats(t.data_stats), t.job_id, t.p4_path]
        _write_row(row)
        i += 1
        for metric in t.test_metrics:
            row = ["SMRT Link - SA", "End-to-end Pipeline", t.pipeline,
                   t.data_type, t.description,
                   "Reports: {m}".format(m=metric), "YES", level, "", "",
                   _get_subread_set(t.entry_points),
                   format_data_stats(t.data_stats), t.job_id, t.p4_path]
            _write_row(row)
            i += 1
    wb.close()
